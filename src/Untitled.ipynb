{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bdc9dcb-cada-4a37-926b-2ac98508ab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\athul\\anaconda3\\envs\\mini_project\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in c:\\users\\athul\\anaconda3\\envs\\mini_project\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\athul\\anaconda3\\envs\\mini_project\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\athul\\anaconda3\\envs\\mini_project\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\athul\\anaconda3\\envs\\mini_project\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\athul\\anaconda3\\envs\\mini_project\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\athul\\anaconda3\\envs\\mini_project\\lib\\site-packages (from scikit-learn) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn scipy pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d14d45-dd9c-4b62-9f21-4fed35074ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        print(\"loading DeepNet (Inception-V3) ...\")\n",
    "        base_model = InceptionV3(weights='imagenet')\n",
    "        \n",
    "        # Define the model up to the second to last layer\n",
    "        self.model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "     \n",
    "    def get_features(self, batch):\n",
    "        features =  self.model.predict(batch)\n",
    "        features = features.reshape(-1, features.shape[-1])\n",
    "        return normalize(features, axis=1, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c06a9f2-5605-4d38-ba38-3bbc23f0b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 25 Dec 2016\n",
    "\n",
    "@author: Morris Franken\n",
    "Loads a batch of images and prepares them for forwarding into a keras deep net.\n",
    "'''\n",
    "import numpy as np\n",
    "from multiprocessing.pool import Pool\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3  import preprocess_input\n",
    "\n",
    "def loadImage(path):\n",
    "    img = image.load_img(path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def loadBatch(img_paths):\n",
    "    with Pool(processes=8) as pool:\n",
    "        imgs = pool.map(loadImage, img_paths)\n",
    "        return np.vstack(imgs)\n",
    "\n",
    "# Use this for training, instead of loading everything into memory, in only loads chunks\n",
    "def batchGenerator(img_paths, labels, batch_size):\n",
    "    for i in range(0, len(img_paths), batch_size):\n",
    "        batch_paths = img_paths[i:(i + batch_size)]\n",
    "        batch_labels = labels[i:(i + batch_size)]\n",
    "        batch_images = loadBatch(batch_paths)\n",
    "        yield batch_images, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d463d870-d9d6-4fc3-ba82-9a082fa151e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'featureExtractor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZipFile\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeatureExtractor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractor\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimageLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m batchGenerator\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# setup all the paths adn variables\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'featureExtractor'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "'''\n",
    "Created on 6 Jan 2017\n",
    "\n",
    "@author: Morris Franken\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from os import listdir, path\n",
    "from os.path import isdir, isfile, join, exists, dirname\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import sklearn.externals\n",
    "import joblib\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "from featureExtractor import FeatureExtractor\n",
    "from imageLoader import batchGenerator\n",
    "\n",
    "# setup all the paths adn variables\n",
    "file_dir         = dirname(__file__)\n",
    "dataPath         = join(file_dir, \"../Dataset/\")\n",
    "stelePath        = join(dataPath, \"Manual/Preprocessed\")\n",
    "intermediatePath = join(file_dir, \"../intermediates\")\n",
    "featurePath      = join(intermediatePath, \"features.npy\")\n",
    "labelsPath       = join(intermediatePath, \"labels.npy\")\n",
    "svmPath          = join(intermediatePath, \"svm.pkl\")\n",
    "image_paths      = []\n",
    "labels           = []\n",
    "batch_size       = 200\n",
    "if not exists(dataPath):\n",
    "    print(\"downloading dataset (57.5MB)\")\n",
    "    url = urlopen(\"http://iamai.nl/downloads/GlyphDataset.zip\")    \n",
    "    with ZipFile(BytesIO(url.read())) as z:\n",
    "        z.extractall(join(dataPath, \"..\"))\n",
    "\n",
    "# check if the feature file is present, if so; there is no need to recompute the features\n",
    "# The pre-computed features can also be downloaded from http://iamai.nl/downloads/features.npy\n",
    "if not isfile(featurePath):\n",
    "    print(\"indexing images...\")\n",
    "    Steles = [ join(stelePath,f) for f in listdir(stelePath) if isdir(join(stelePath,f)) ]\n",
    "    for stele in Steles:    \n",
    "        imagePaths = [ join(stele,f) for f in listdir(stele) if isfile(join(stele,f)) ]\n",
    "        for path in imagePaths:\n",
    "            image_paths.append(path)\n",
    "            labels.append(path[(path.rfind(\"_\") + 1): path.rfind(\".\")])\n",
    "    \n",
    "    featureExtractor = FeatureExtractor()\n",
    "    features = []\n",
    "    print(\"computing features...\")\n",
    "    for idx, (batch_images, _) in enumerate(batchGenerator(image_paths, labels, batch_size)):\n",
    "        print(\"{}/{}\".format((idx+1) * batch_size, len(labels)))\n",
    "        features_ = featureExtractor.get_features(batch_images)\n",
    "        features.append(features_)\n",
    "    features = np.vstack(features)\n",
    "    \n",
    "    labels = np.asarray(labels)\n",
    "    print(\"saving features...\")\n",
    "    np.save(featurePath, features)\n",
    "    np.save(labelsPath, labels)\n",
    "else:\n",
    "    print(\"loading precomputed features and labels from {} and {}\".format(featurePath, labelsPath))\n",
    "    features = np.load(featurePath)\n",
    "    labels = np.load(labelsPath)\n",
    "\n",
    "# on to the SVM trainign phase\n",
    "tobeDeleted = np.nonzero(labels == \"UNKNOWN\") # Remove the Unknown class from the database\n",
    "features = np.delete(features,tobeDeleted, 0)\n",
    "labels = np.delete(labels,tobeDeleted, 0)\n",
    "numImages = len(labels)\n",
    "trainSet, testSet, trainLabels, testLabels = train_test_split(features, labels, test_size=0.20, random_state=42) \n",
    "\n",
    "# Training SVM, feel free to use linear SVM (or another classifier for that matter) for faster training, however that will not give the confidence scores that can be used to rank hieroglyphs\n",
    "print(\"training SVM...\")\n",
    "if 0: # optinal; either train 1 classifier fast, or search trough the parameter space by training multiple classifiers to sqeeze out that extra 2%\n",
    "    clf = linear_model.LogisticRegression(C=10000)\n",
    "else:\n",
    "    svr = linear_model.LogisticRegression()\n",
    "    parameters = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "    clf = GridSearchCV(svr, parameters, n_jobs=8)\n",
    "clf.fit(trainSet, trainLabels)\n",
    "    \n",
    "print(clf)\n",
    "print(\"finished training! saving...\")\n",
    "joblib.dump(clf, svmPath, compress=1) \n",
    "\n",
    "prediction = clf.predict(testSet)\n",
    "accuracy = np.sum(testLabels == prediction) / float(len(prediction))\n",
    "\n",
    "# for idx, pred in enumerate(prediction):\n",
    "#     print(\"%-5s --> %s\" % (testLabels[idx], pred))\n",
    "print(\"accuracy = {}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d55310-3a55-4756-80ab-403a62fc722b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
